{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "308c4136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.calibration import LabelEncoder\n",
    "import numpy as np\n",
    "# 1. Load the data\n",
    "df = pd.read_csv('./megablunders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc872b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "PR            14\n",
      "PAR           14\n",
      "ROS           14\n",
      "MM            14\n",
      "DM            13\n",
      "AGREE         13\n",
      "CASE          13\n",
      "FRAG          12\n",
      "NONE          11\n",
      "AGREEERROR     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution after dropping:\n",
      "error\n",
      "PR       14\n",
      "PAR      14\n",
      "ROS      14\n",
      "MM       14\n",
      "DM       13\n",
      "AGREE    13\n",
      "CASE     13\n",
      "FRAG     12\n",
      "NONE     11\n",
      "Name: count, dtype: int64\n",
      "error\n",
      "PR       14\n",
      "PAR      14\n",
      "ROS      14\n",
      "MM       14\n",
      "DM       13\n",
      "AGREE    13\n",
      "CASE     13\n",
      "FRAG     12\n",
      "NONE     11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# look at class imbalances\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get the count for each label\n",
    "print(df[\"error\"].value_counts())\n",
    "\n",
    "df_filtered = df[df[\"error\"] != \"AGREEERROR\"]\n",
    "\n",
    "# Verify the class has been removed\n",
    "print(\"\\nClass distribution after dropping:\")\n",
    "print(df_filtered[\"error\"].value_counts())\n",
    "\n",
    "# Update your DataFrame\n",
    "df = df_filtered\n",
    "\n",
    "\n",
    "print(df[\"error\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e261f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use lable encoder to encode the error\n",
    "label_encoder = LabelEncoder()\n",
    "df['error'] = label_encoder.fit_transform(df['error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0374c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  vectorize the text \n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "st = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e2ea997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list of strings before encoding\n",
    "sentences = df[\"original_sentence\"].tolist()\n",
    "embeddings = st.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "37ee44e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"error\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f19a7078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_sentence</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>By dropping a game to the pathetic Tampa Bay D...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Although, if history is any indication, the te...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Sox almost never go down uneventfully, whi...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Because of the accumulated bad karma that hang...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The team not only has squandered huge leads bu...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   original_sentence  error\n",
       "0  By dropping a game to the pathetic Tampa Bay D...      2\n",
       "1  Although, if history is any indication, the te...      3\n",
       "2  The Sox almost never go down uneventfully, whi...      7\n",
       "3  Because of the accumulated bad karma that hang...      0\n",
       "4  The team not only has squandered huge leads bu...      6"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4f67f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, df['error'], test_size=0.1, random_state=42, stratify=df[\"error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb05573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/felixstuart/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/felixstuart/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the necessary NLTK data first\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Check if the data was downloaded successfully\n",
    "try:\n",
    "    from nltk.corpus import wordnet\n",
    "    print(\"WordNet loaded successfully\")\n",
    "except:\n",
    "    print(\"WordNet loading failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3972d787",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordnet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# use nplaug to expand the dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnlpaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmenter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mword\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnaw\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m syn_aug \u001b[38;5;241m=\u001b[39m \u001b[43mnaw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSynonymAug\u001b[49m\u001b[43m(\u001b[49m\u001b[43maug_src\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwordnet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m augmented_sentences \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m augmented_labels \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Documents/School/Clubs/Programming Club/Potential Projects/ML Megablunders/.venv/lib/python3.9/site-packages/nlpaug/augmenter/word/synonym.py:66\u001b[0m, in \u001b[0;36mSynonymAug.__init__\u001b[0;34m(self, aug_src, model_path, name, aug_min, aug_max, aug_p, lang, stopwords, tokenizer, reverse_tokenizer, stopwords_regex, force_reload, verbose)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path \u001b[38;5;241m=\u001b[39m model_path\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang \u001b[38;5;241m=\u001b[39m lang\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43maug_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/School/Clubs/Programming Club/Potential Projects/ML Megablunders/.venv/lib/python3.9/site-packages/nlpaug/augmenter/word/synonym.py:163\u001b[0m, in \u001b[0;36mSynonymAug.get_model\u001b[0;34m(cls, aug_src, lang, dict_path, force_reload)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_model\u001b[39m(\u001b[38;5;28mcls\u001b[39m, aug_src, lang, dict_path, force_reload):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m aug_src \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnmw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWordNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_synonym\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m aug_src \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppdb\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m init_ppdb_model(dict_path\u001b[38;5;241m=\u001b[39mdict_path, force_reload\u001b[38;5;241m=\u001b[39mforce_reload)\n",
      "File \u001b[0;32m~/Documents/School/Clubs/Programming Club/Potential Projects/ML Megablunders/.venv/lib/python3.9/site-packages/nlpaug/model/word_dict/wordnet.py:33\u001b[0m, in \u001b[0;36mWordNet.__init__\u001b[0;34m(self, lang, is_synonym)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissed nltk library. Install nltk by `pip install nltk`\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#     # Check whether wordnet package is downloaded\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#     wordnet.synsets('computer')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#     nltk.download('wordnet')\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#     nltk.download('averaged_perceptron_tagger')\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/School/Clubs/Programming Club/Potential Projects/ML Megablunders/.venv/lib/python3.9/site-packages/nlpaug/model/word_dict/wordnet.py:37\u001b[0m, in \u001b[0;36mWordNet.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m         \u001b[43mwordnet\u001b[49m\u001b[38;5;241m.\u001b[39msynsets(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m wordnet\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wordnet' is not defined"
     ]
    }
   ],
   "source": [
    "# use nplaug to expand the dataset\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "syn_aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "augmented_sentences = []\n",
    "augmented_labels = []\n",
    "\n",
    "for sentence, label in zip(X_train, y_train):\n",
    "    augmented = syn_aug.augment(sentence)\n",
    "    augmented_sentences.append(augmented)\n",
    "    augmented_labels.append(label)\n",
    "    \n",
    "X_train_extended = X_train + augmented_sentences\n",
    "y_train_extended = y_train + augmented_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de22ea23",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_extended' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 23\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Fit the XGBoost model with sample weights\u001b[39;00m\n\u001b[1;32m     13\u001b[0m xboost \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m     14\u001b[0m                            learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m     15\u001b[0m                            random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m                            scale_pos_weight\u001b[38;5;241m=\u001b[39msample_weights,\n\u001b[1;32m     20\u001b[0m                            )\n\u001b[0;32m---> 23\u001b[0m xboost\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train_extended\u001b[49m, y_train_extended, sample_weight\u001b[38;5;241m=\u001b[39msample_weights)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m     26\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m xboost\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_extended' is not defined"
     ]
    }
   ],
   "source": [
    "# try xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert class weights to sample weights\n",
    "# Create a dictionary mapping class labels to weights\n",
    "class_weight_dict = dict(zip(unique_classes, weights))\n",
    "\n",
    "# Map each sample to its appropriate weight\n",
    "sample_weights = np.array([class_weight_dict[cls] for cls in y_train])\n",
    "\n",
    "# Fit the XGBoost model with sample weights\n",
    "xboost = xgb.XGBClassifier(n_estimators=500,\n",
    "                           learning_rate=0.01,\n",
    "                           random_state=42,\n",
    "                           reg_alpha=0.7,\n",
    "                           reg_lambda=1,\n",
    "                           max_depth=3,\n",
    "                           scale_pos_weight=sample_weights,\n",
    "                           )\n",
    "\n",
    "\n",
    "xboost.fit(X_train_extended, y_train_extended, sample_weight=sample_weights)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xboost.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"XGBoost Accuracy: {accuracy*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
